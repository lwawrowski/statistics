---
title: "Linear regression"
author: "Łukasz Wawrowski"
output: html_document
---

The purpose of regression analysis is to determine, on the basis of the available information, the unknown value of the analyzed variable. We know the sales values and the number of customers in a given Rossmann store and would like to determine the possible level of sales for a given number of customers, e.g. 1000 customers. 

Similarly as in the correlation analysis, the starting point in a simple regression is to create a scatter plot. In order to apply the regression model, there must be a correlation relationship between the variables and it must be a linear relationship. Then we have to determine the dependent variable ($y$) and the independent variable ($x$).

Consider a simple example of income and spendings:

```{r include=FALSE}
knitr::opts_chunk$set(cache = FALSE, echo = FALSE, message = F, error = F)
```


```{r}
library(tidyverse)

options(scipen = 999)

load("data/dane.RData")

rossmann_1 <- rossmann %>% 
  filter(sklep_id == 1, czy_otwarty == "Tak")

d <- data.frame(spendings=c(2300,1800,2400,2300,2800,2000,2100),
                income=c(2600,2400,2900,2800,3000,2500,2700))

d %>% knitr::kable()
```

The relationship between spendings and income seems obvious - for $y$ we will take spendings and $x$ is income. In the chart, this dependence is as follows:

```{r}
ggplot(d, aes(x = income, y = spendings)) + 
  geom_point(size = 2) +
  xlab("income (X)") + 
  ylab("spendings (Y)") +
  xlim(2400,3000) +
  ylim(1600,3100) +
  theme_light()
```

We are interested in creating a model that simplifies reality to the level of a formula for a simple line, whose general equation is as follows:

$$y=a_1\cdot x+a_0$$

For only two points, determining the $a_1$ and $a_0$ coefficients would not be a problem. However, for the given example, the Ordinary Least Squares (OLS) method should be used, in which we minimize the distance of points from the line. 

Now let's try to fit a few lines.

```{r}
ggplot(d, aes(x = income, y = spendings)) + 
  geom_point(size = 2) +
  geom_hline(yintercept = 2243, color = "blue", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.9, intercept = -2780, color = "green", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.357, intercept = -1421.429, color = "red", alpha = 0.8, size = 1.1) +
  xlab("income (X)") + 
  ylab("spendings (Y)") +
  xlim(2400,3000) +
  ylim(1600,3100) +
  theme_light()
```

In the next step we calculate the differences between the existing points and the corresponding values on the line: 

```{r}
d <- d %>% 
  mutate(niebieska_y=income-2243,
         zielona_y=1.9*income-2780,
         czerwona_y=1.357*income-1421.429) %>% 
  mutate(blue=(spendings-niebieska_y)^2,
         green=(spendings-zielona_y)^2,
         red=(spendings-czerwona_y)^2)

line1 <- data.frame(x = c(d$income[1], d$income[1]), y=c(d$spendings[1], d$czerwona_y[1]))
line2 <- data.frame(x = c(d$income[2], d$income[2]), y=c(d$spendings[2], d$czerwona_y[2]))
line3 <- data.frame(x = c(d$income[3], d$income[3]), y=c(d$spendings[3], d$czerwona_y[3]))
line4 <- data.frame(x = c(d$income[4], d$income[4]), y=c(d$spendings[4], d$czerwona_y[4]))
line5 <- data.frame(x = c(d$income[5], d$income[5]), y=c(d$spendings[5], d$czerwona_y[5]))
line6 <- data.frame(x = c(d$income[6], d$income[6]), y=c(d$spendings[6], d$czerwona_y[6]))
line7 <- data.frame(x = c(d$income[7], d$income[7]), y=c(d$spendings[7], d$czerwona_y[7]))

ggplot(d, aes(x = income, y = spendings)) + 
  geom_point(size = 2) +
  geom_hline(yintercept = 2243, color = "blue", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.9, intercept = -2780, color = "green", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.357, intercept = -1421.429, color = "red", alpha = 0.8, size = 1.1) +
  geom_line(data = line1, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line2, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line3, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line4, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line5, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line6, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line7, aes(x=x, y=y), color = "grey60", size = 1.2) +
  xlab("income (X)") + 
  ylab("spendings (Y)") +
  xlim(2400,3000) +
  ylim(1600,3100) +
  theme_light()
```

By denoting $y_i$ as the actual value of spendings and $\hat{y_i}$ as the value lying on the line we want to minimize the expression $\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2} \rightarrow min$. The difference $y_{i}-\hat{y}_{i}$ is called residual. By calulating these values for the analyzed lines, we will get the following results:

```{r}
d %>% 
  select(6:8) %>% 
  pivot_longer(1:3) %>% 
  group_by(name) %>% 
  summarise(sum_squares=round(sum(value))) %>% 
  ungroup() %>% 
  rename(line=name) %>% 
  arrange(sum_squares) %>% 
  knitr::kable()
```

As we can see, the smallest value of the sum of squares is observed for the red line. We are now interested in the equation of this line. Assuming the earlier symbols, the general form of the regression line is as follows:

$$\hat{y}_{i}=a_{1}x_{i}+a_{0}$$

where $y$ with a hat ($\hat{y}$ is a theoretical value, lying on a line. 

Therefore the empirical/real values (y) will be described by the formula:

$$y_{i}=a_{1}x_{i}+a_{0}+u_{i}$$

where $u_i$ is the residual calculated as $u_{i}=y_{i}-\hat{y}_{i}$. 

In the Rossmann case the dependent variable will be the sales ($y$), which will be explained by the number of customers ($x$). Our goal is to find a formula of a straight line that will be as close as possible to all points of the plot. We need to determine the slope ($a_1$) and the point of intersection with the OY axis ($a_0$) - intercept or constant.

```{r ross-reg, echo=FALSE, fig.height=5, fig.width=8, warning=FALSE, message=FALSE}

ggplot(rossmann_1, aes(y=sprzedaz, x=liczba_klientow)) + 
  geom_smooth(method = "lm", se = F, colour="grey80") + 
  geom_point() +
  ylab("Sales") + xlab("Number of clients") +
  theme_bw()

```

The values of these coefficients can be calculated using the following formula:

$$
a_{1}=\frac{\sum\limits_{i=1}^{n}{(x_{i}-\bar{x})(y_{i}-\bar{y})}}{\sum\limits_{i=1}^{n}{(x_{i}-\bar{x})^{2}}}
$$

or knowing Pearson's linear correlation coefficient:

$$a_{1}=r\frac{S_{y}}{S_{x}}$$

The value of the intercept can be obtained from the formula:

$$a_{0}=\bar{y}-a_{1}\bar{x}$$

where:

- $r$ - Pearson linear correlation coefficient for $x$ and $y$,
- $S_y$ - standard deviation of $y$,
- $S_x$ - standard deviation of  $x$,
- $\bar{y}$ - an average of $y$,
- $\bar{x}$ - an average of $x$.

On this basis, we determine that the straight line we are interested in has the following formula:

$$\hat{y}_i=10,45x_i-1091,22$$

A slope ($a_1$) indicates how much the average value of the dependent variable (y) will change when the value of the independent variable (x) increases by a unit. In other words: as the value of the x variable increses by 1 the value of the y increses by a slope. 

In our example, an increase in the number of customers by 1 person will result in an average sales increase of 10.45 EUR. 

The intercept ($a_0$) is the value of the dependent variable (y), if the value of the independent variable (x) is 0. Special care should be taken when interpreting this coefficient, because it is often meaningless. If x variable never is equal to 0, there is no interest in the intercept.

In the analyzed example, the $a_0$ coefficient indicates that with zero customers, sales in store 1 will be -1091,22 euros, which have no sense.

## Regression assessment

Another element of the regression analysis is the assessment of model fitting. For this purpose, we use several measures.

The first measure that describes fitting the regression function to the empirical data is the **standard error of the regression**, which is the square root of the sum of the squares of the residuals divided by the number of observations minus 2. This reduction of the denominator is due to the fact that in the model we have two coefficients $a_1$ and $a_0$, which we take into account in this way. Formally this can be written as follows: 

$$S_{u}=\sqrt{\frac{\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2}}{n-2}}$$
or

$$S_{u}=\sqrt{\frac{\sum\limits_{i=1}^{n}{u_i^2}}{n-2}}$$

The standard error of the regression represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the dependent variable. Smaller values are better because it indicates that the observations are closer to the fitted line.

In the analyzed case we can state that the known sales values deviate from the theoretical values by an average of $+/-$ 351,57 EUR. 

The standard error of the regression is also a measure of forecast error. For example, we want to check how mushc sales will be obtain when there are 1000 customers. After substituting this value for the regression function we will get it:

$$y_{1000}=10,45 \cdot 1000 - 1091,22=9358,78$$

On this basis, we conclude that with 1,000 customers, the sales forecast would be EUR 9358,78 $+/-$ 351,57.

Kolejna miara to **współczynnik zmienności resztowej**, który otrzymujemy poprzez podzielenie odchylenia standardowego składnika resztowego przez średni poziom cechy:

$$V_{u}=\frac{S_{u}}{\bar{y}}\cdot 100\%$$

Współczynnik ten wskazuje, jaki procent średniego poziomu zmiennej objaśnianej stanowią wahania losowe, których miarą jest $S_{u}$. Parametr $V_{u}$ jest więc miernikiem relatywnej wielkości błędu losowego. Niektórzy autorzy postulują, że błąd ten można umownie uznać za dopuszczalny, jeśli $V_{u}<15\%$. Należy się jednak wystrzegać przed ,,dogmatycznym'' podejściem do oceny modeli regresji i jedynie słusznych progów. 

W naszym przypadku ten współczynnik będzie równy $V_{u}=\frac{351,57}{4730,72}\cdot 100\%=7\%$ co oznacza, że 7% średniego poziomu sprzedaży stanowią wahania losowe.

Równie ważną miarą dopasowania funkcji regresji do danych empirycznych jest **współczynnik determinacji** lub bardziej potocznie **współczynnik r kwadrat** --- od symbolu, którym jest oznaczany. Współczynnik ten obliczany jest na podstawie reszt z modelu oraz odchyleń wartości empirycznych od średniej:

$$R^2=1-\frac{\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2}}{\sum\limits_{i=1}^{n}{(y_{i}-\bar{y}_{i})^2}}$$

lub jako kwadrat współczynnika korelacji liniowej Pearsona:

$$R^2=r_{xy}^2$$

Określa, jaki procent wariancji zmiennej objaśnianej został wyjaśniony przez funkcję regresji. $R^2$ przyjmuje wartości z przedziału $<0;1>$ ($<0\%;100\%>$), przy czym model regresji tym lepiej opisuje zachowanie się badanej zmiennej objaśnianej, im $R^2$ jest bliższy jedności (bliższy 100\%)

Analizowany przez nas model regresji jest bardzo dobry: $R^2=0,89$, co oznacza, że oszacowany model regresji wyjaśnia 89% zmienności sprzedaży. 

Przeciwieństwem współczynnik determinacji $R^2$ jest **współczynnik zbieżności (indeterminacji)**. Tę miarę można wyznaczyć korzystając ze wzoru:

$$\varphi^2=\frac{\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2}}{\sum\limits_{i=1}^{n}{(y_{i}-\bar{y}_{i})^2}}$$

bądź odejmując od wartości 1 wartość współczynnika $R^2$: 

$$\varphi^2=1-R^2$$

Współczynnik zbieżności $\varphi^2$ określa, jaka część wariancji badanej zmiennej objaśnianej nie została wyjaśniona przez funkcję regresji. Oczywiste jest więc, że korzystna sytuacja występuje wówczas, gdy $\varphi^2$ jest bliższy zera.

W przyjętym przez nas modelu regresji $\phi^2=11\%$, co oznacza, że 11% zmienności sprzedaży nie została wyjaśniona przez funkcję regresji. Można także powiedzieć, że 11% zmienności sprzedaży stanowią czynniki losowe nie wyjaśniane przez funkcję regresji.

Ostatnim elementem analizy jest ocena jakości parametrów funkcji regresji $a_1$ i $a_0$. Równanie regresji wyznaczyliśmy na podstawie dostępnych danych, ale nie znamy równania tej prostej w populacji. W związku z czym mogliśmy się trochę pomylić przy obliczaniu współczynników $a_1$ i $a_0$. W celu oceny skali tych błędów wyznacza się **błędy średnie szacunku ocen parametrów funkcji regresji** według wzorów:

$$S_{a_{1}}=\frac{S_{u}}{\sqrt{\sum\limits_{i=1}^{n}{(x_{i}-\bar{x})^2}}}$$

oraz 

$$S_{a_{0}}=\sqrt{\frac{S_{u}^{2}\sum\limits_{i=1}^{n}{{x_{i}^2}}}{n\sum\limits_{i=1}^{n}{(x_{i}-\bar{x})^2}}}$$

Błędy te wskazują, o ile, przeciętnie biorąc ($+/-$), odchylają się oceny parametrów modelu regresji od ich wartości prawdziwych. Jest oczywiście pożądane, żeby te błędy były możliwie jak najmniejsze. W związku z powyższym przyjmuje się, że ilorazy:

$$V_{a_{1}}=\frac{S_{a_{1}}}{a_{1}}$$

$$V_{a_{0}}=\frac{S_{a_{0}}}{a_{0}}$$

nie powinny przekraczać wartości 0,5 (50\%) w wartości bezwzględnej. 

Jest to szczególnie istotne w przypadku parametru współczynnika kierunkowego $a_1$, natomiast dla wyrazu wolnego $a_0$ ta własność nie musi być spełniona.

W analizowanym przez nas modelu wartość parametru $a_1$ odchyla się od jego wartości prawdziwej o $+/-$ 0,21 co stanowi 2% wartości tego parametru. Z kolei wartość parametru $a_0$ odchyla się od jego wartości prawdziwej o $+/-$ 119,81 co stanowi 11% wartości tego parametru.

**Regresja prosta w Excelu**

- **Sposób nr 1**

Parametry funkcji regresji można także wyznaczyć korzystając z wbudowanej funkcji programu Excel --- REGLINP. Składnia jest następująca:

- REGLINP(wektor_y; wektor_x; stała; statystyka)

gdzie:

- wektor_y --- zestaw wartości zmiennej objaśnianej (y),
- wektor_x --- zestaw wartości zmiennej objaśniającej (y),
- stała --- jeśli podamy wartość 1 to wyraz wolny jest obliczany normalnie, jeśli podamy 0 to zostanie oszacowany model bez wyrazu wolnego,
- statystyka --- jeśli argument ma wartość 1 to funkcja REGLINP zwraca dodatkowe statystyki regresji, natomiast jeśli ma wartość 0 to funkcja zwraca tylko wartości współczynnika kierunkowego oraz wyrazu wolnego.

Po napisaniu funkcji i uwzględnieniu wszystkich argumentów naciskamy ENTER --- powinna pojawić się jedna wartość. Następnie należy zaznaczyć obszar 2 kolumny na 5 wiersze uwzględniając w lewej górnej komórce otrzymaną wcześniej wartość. W kolejnym kroku przechodzimy do PASKU FORMUŁY programu Excel i korzystamy z tajemnej formuły CTRL+SHIFT+ENTER.

W rezultacie otrzymujemy tabelę o wymiarach 2x5, która zawiera następujące elementy:

|                                                         |                                                       |
|---------------------------------------------------------|-------------------------------------------------------|
| Współczynnik kierunkowy ($a_{1}$)                       | Wyraz wolny ($a_{0}$)                                 |
| Średni błąd szacunku parametru ($S_{a_{1}}$)            | Średni błąd szacunku parametru ($S_{a_{0}}$)          |
| Współczynnik determinacji ($R^2$)                       | Odchylenie standardowe składnika resztowego ($S_{u}$) |
| Statystyka F ($F$)                                      | Liczba stopni swobody ($n-2$)                         |
| Regresyjna suma kwadratów ($\sum{(\hat{y}-\bar{y})^2}$) | Suma kwadratów reszt ($\sum{(y-\hat{y})^2}$)          |

- **Sposób nr 2**

Zaznaczamy punkty na wykresie rozrzutu i klikamy prawym przyciskiem myszy. Wybieramy **Dodaj linię trendu**, a następnie zaznaczamy opcje Wyświetl równanie na wykresie oraz Wyświetl wartości R-kwadrat na wykresie.

- **Sposób nr 3**

Do wyznaczenia parametrów regresji można także wykorzystać graficzne środowisko analizy danych. W tych celu wybieramy zakładkę DANE i po prawej stronie ANALIZA DANYCH. W menu zaznaczamy REGRESJA i klikamy OK. W opcjach wejścia zaznaczamy:

- Zakres wejściowy Y  --- zestaw wartości zmiennej objaśnianej (y),
- Zakres wejściowy X --- zestaw wartości zmiennej objaśniającej (x),
- Tytuły --- jeśli zostały zaznaczone kolumny wraz z nagłówkami.

W opcjach wyjścia określamy miejsce wyświetlenia wyniku: bieżący arkusz/nowy arkusz/nowy skoroszyt.

W rezultacie otrzymujemy następujący wynik:


PODSUMOWANIE - WYJŚCIE

| Statystyki regresji  |         |
|----------------------|---------|
| Wielokrotność R      | $r$     |
| R kwadrat            | $R^2$   |
| Dopasowany R kwadrat |         |
| Błąd standardowy     | $S_{u}$ |
| Obserwacje           | $n$     |

ANALIZA WARIANCJI

|       | df    |    SS       | MS          |F   | Istotność F            |
|-----------|-----|-------------|-------------|-------------|-------------|
| Regresja  | 1   | $\sum{(\hat{y}-\bar{y})^2}$ |  | $F$ |  |
| Resztkowy | $n-2$ | $\sum{(y-\hat{y})^2}$ |  |             |             |
| Razem     | $n-1$ |  |             |             |             |

|        | Współczynniki |Błąd standardowy       | t Stat     | Wartość-p    |  
|---------------------|------------------|-------------|--------------|-------------|
| Przecięcie          | $a_{0}$     | $S_{a_{0}}$ |  |  |
| zmienna x | $a_{1}$      | $S_{a_{1}}$ |   |  |

Zależności:

- Jeżeli ze wzoru na odchylenie standardowe składnika resztowego usuniemy pierwiastek to otrzymamy wariancję składnika resztowego, którą należy najpierw spierwiastkować, aby móc przeprowadzić interpretację.

- Jeśli bardzo chcemy policzyć wartość odchylenia standardowe składnika resztowego na podstawie wartości surowych to wartość licznika możemy odczytać z funkcji REGLINP --- 5 wiersz, 2 kolumna. Wówczas wystarczy podzielić tę wartość przez 4 wiersz drugiej kolumny i spierwiastkować, aby otrzymać wartość $S_u$. Podobnie postępujemy, jeśli korzystamy z narzędzia REGRESJA.

