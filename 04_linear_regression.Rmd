---
title: "Linear regression"
author: "≈Åukasz Wawrowski"
output: html_document
---

The purpose of regression analysis is to determine, on the basis of the available information, the unknown value of the analyzed variable. We know the sales values and the number of customers in a given Rossmann store and would like to determine the possible level of sales for a given number of customers, e.g. 1000 customers. 

Similarly as in the correlation analysis, the starting point in a simple regression is to create a scatter plot. In order to apply the regression model, there must be a correlation relationship between the variables and it must be a linear relationship. Then we have to determine the dependent variable ($y$) and the independent variable ($x$).

Consider a simple example of income and spendings:

```{r include=FALSE}
knitr::opts_chunk$set(cache = FALSE, echo = FALSE, message = F, error = F)
```


```{r}
library(tidyverse)

options(scipen = 999)

load("data/dane.RData")

rossmann_1 <- rossmann %>% 
  filter(sklep_id == 1, czy_otwarty == "Tak")

d <- data.frame(spendings=c(2300,1800,2400,2300,2800,2000,2100),
                income=c(2600,2400,2900,2800,3000,2500,2700))

d %>% knitr::kable()
```

The relationship between spendings and income seems obvious - for $y$ we will take spendings and $x$ is income. In the chart, this dependence is as follows:

```{r}
ggplot(d, aes(x = income, y = spendings)) + 
  geom_point(size = 2) +
  xlab("income (X)") + 
  ylab("spendings (Y)") +
  xlim(2400,3000) +
  ylim(1600,3100) +
  theme_light()
```

We are interested in creating a model that simplifies reality to the level of a formula for a simple line, whose general equation is as follows:

$$y=a_1\cdot x+a_0$$

For only two points, determining the $a_1$ and $a_0$ coefficients would not be a problem. However, for the given example, the Ordinary Least Squares (OLS) method should be used, in which we minimize the distance of points from the line. 

Now let's try to fit a few lines.

```{r}
ggplot(d, aes(x = income, y = spendings)) + 
  geom_point(size = 2) +
  geom_hline(yintercept = 2243, color = "blue", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.9, intercept = -2780, color = "green", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.357, intercept = -1421.429, color = "red", alpha = 0.8, size = 1.1) +
  xlab("income (X)") + 
  ylab("spendings (Y)") +
  xlim(2400,3000) +
  ylim(1600,3100) +
  theme_light()
```

In the next step we calculate the differences between the existing points and the corresponding values on the line: 

```{r}
d <- d %>% 
  mutate(niebieska_y=income-2243,
         zielona_y=1.9*income-2780,
         czerwona_y=1.357*income-1421.429) %>% 
  mutate(blue=(spendings-niebieska_y)^2,
         green=(spendings-zielona_y)^2,
         red=(spendings-czerwona_y)^2)

line1 <- data.frame(x = c(d$income[1], d$income[1]), y=c(d$spendings[1], d$czerwona_y[1]))
line2 <- data.frame(x = c(d$income[2], d$income[2]), y=c(d$spendings[2], d$czerwona_y[2]))
line3 <- data.frame(x = c(d$income[3], d$income[3]), y=c(d$spendings[3], d$czerwona_y[3]))
line4 <- data.frame(x = c(d$income[4], d$income[4]), y=c(d$spendings[4], d$czerwona_y[4]))
line5 <- data.frame(x = c(d$income[5], d$income[5]), y=c(d$spendings[5], d$czerwona_y[5]))
line6 <- data.frame(x = c(d$income[6], d$income[6]), y=c(d$spendings[6], d$czerwona_y[6]))
line7 <- data.frame(x = c(d$income[7], d$income[7]), y=c(d$spendings[7], d$czerwona_y[7]))

ggplot(d, aes(x = income, y = spendings)) + 
  geom_point(size = 2) +
  geom_hline(yintercept = 2243, color = "blue", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.9, intercept = -2780, color = "green", alpha = 0.8, size = 1.1) +
  geom_abline(slope = 1.357, intercept = -1421.429, color = "red", alpha = 0.8, size = 1.1) +
  geom_line(data = line1, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line2, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line3, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line4, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line5, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line6, aes(x=x, y=y), color = "grey60", size = 1.2) +
  geom_line(data = line7, aes(x=x, y=y), color = "grey60", size = 1.2) +
  xlab("income (X)") + 
  ylab("spendings (Y)") +
  xlim(2400,3000) +
  ylim(1600,3100) +
  theme_light()
```

By denoting $y_i$ as the actual value of spendings and $\hat{y_i}$ as the value lying on the line we want to minimize the expression $\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2} \rightarrow min$. The difference $y_{i}-\hat{y}_{i}$ is called residual. By calulating these values for the analyzed lines, we will get the following results:

```{r}
d %>% 
  select(6:8) %>% 
  pivot_longer(1:3) %>% 
  group_by(name) %>% 
  summarise(sum_squares=round(sum(value))) %>% 
  ungroup() %>% 
  rename(line=name) %>% 
  arrange(sum_squares) %>% 
  knitr::kable()
```

As we can see, the smallest value of the sum of squares is observed for the red line. We are now interested in the equation of this line. Assuming the earlier symbols, the general form of the regression line is as follows:

$$\hat{y}_{i}=a_{1}x_{i}+a_{0}$$

where $y$ with a hat ($\hat{y}$ is a theoretical value, lying on a line. 

Therefore the empirical/real values (y) will be described by the formula:

$$y_{i}=a_{1}x_{i}+a_{0}+u_{i}$$

where $u_i$ is the residual calculated as $u_{i}=y_{i}-\hat{y}_{i}$. 

In the Rossmann case the dependent variable will be the sales ($y$), which will be explained by the number of customers ($x$). Our goal is to find a formula of a straight line that will be as close as possible to all points of the plot. We need to determine the slope ($a_1$) and the point of intersection with the OY axis ($a_0$) - intercept or constant.

```{r ross-reg, echo=FALSE, fig.height=5, fig.width=8, warning=FALSE, message=FALSE}

ggplot(rossmann_1, aes(y=sprzedaz, x=liczba_klientow)) + 
  geom_smooth(method = "lm", se = F, colour="grey80") + 
  geom_point() +
  ylab("Sales") + xlab("Number of clients") +
  theme_bw()

```

The values of these coefficients can be calculated using the following formula:

$$
a_{1}=\frac{\sum\limits_{i=1}^{n}{(x_{i}-\bar{x})(y_{i}-\bar{y})}}{\sum\limits_{i=1}^{n}{(x_{i}-\bar{x})^{2}}}
$$

or knowing Pearson's linear correlation coefficient:

$$a_{1}=r\frac{S_{y}}{S_{x}}$$

The value of the intercept can be obtained from the formula:

$$a_{0}=\bar{y}-a_{1}\bar{x}$$

where:

- $r$ - Pearson linear correlation coefficient for $x$ and $y$,
- $S_y$ - standard deviation of $y$,
- $S_x$ - standard deviation of  $x$,
- $\bar{y}$ - an average of $y$,
- $\bar{x}$ - an average of $x$.

On this basis, we determine that the straight line we are interested in has the following formula:

$$\hat{y}_i=10,45x_i-1091,22$$

A slope ($a_1$) indicates how much the average value of the dependent variable (y) will change when the value of the independent variable (x) increases by a unit. In other words: as the value of the x variable increses by 1 the value of the y increses by a slope. 

In our example, an increase in the number of customers by 1 person will result in an average sales increase of 10.45 EUR. 

The intercept ($a_0$) is the value of the dependent variable (y), if the value of the independent variable (x) is 0. Special care should be taken when interpreting this coefficient, because it is often meaningless. If x variable never is equal to 0, there is no interest in the intercept.

In the analyzed example, the $a_0$ coefficient indicates that with zero customers, sales in store 1 will be -1091,22 euros, which have no sense.

## Regression assessment

Another element of the regression analysis is the assessment of model fitting. For this purpose, we use several measures.

The first measure that describes fitting the regression function to the empirical data is the **standard error of the regression**, which is the square root of the sum of the squares of the residuals divided by the number of observations minus 2. This reduction of the denominator is due to the fact that in the model we we use 2 means to estimate the slope coefficient (the mean of y and x), which we take into account in this way. Formally this can be written as follows: 

$$S_{u}=\sqrt{\frac{\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2}}{n-2}}$$
or

$$S_{u}=\sqrt{\frac{\sum\limits_{i=1}^{n}{u_i^2}}{n-2}}$$

The standard error of the regression represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the dependent variable. Smaller values are better because it indicates that the observations are closer to the fitted line.

In the analyzed case we can state that the known sales values deviate from the theoretical values by an average of $+/-$ 351,57 EUR. 

The standard error of the regression is also a measure of forecast error. For example, we want to check how mushc sales will be obtain when there are 1000 customers. After substituting this value for the regression function we will get it:

$$y_{1000}=10,45 \cdot 1000 - 1091,22=9358,78$$

On this basis, we conclude that with 1,000 customers, the sales forecast would be EUR 9358,78 $+/-$ 351,57.

An equally important measure of fitting the regression function to the empirical data is the **coefficient of determination** or more commonly, the **R- squared**. The definition of R-squared is fairly straight-forward; it is the percentage of the dependent variable variation that is explained by a linear model.

R-squared is explained variation divided by total variation which can be formally written as: 

$$R^2=1-\frac{\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2}}{\sum\limits_{i=1}^{n}{(y_{i}-\bar{y}_{i})^2}}$$

or as the square of Pearson coefficient of correlation:

$$R^2=r_{xy}^2$$

It determines what percentage of variable variance is explained by the regression function. $R^2$ takes values from $<0;1>$ ($<0\%;100\%>$). In general, the higher the R-squared, the better the model fits your data.

The regression model we are analyzing is very good: $R^2=0,89$, which means that the estimated regression model explains 89% of sales variation. 

The opposite of the coefficient of determination $R^2$ is the **coefficient of non-determination**. This measure can be determined using the formula:

$$\varphi^2=\frac{\sum\limits_{i=1}^{n}{(y_{i}-\hat{y}_{i})^2}}{\sum\limits_{i=1}^{n}{(y_{i}-\bar{y}_{i})^2}}$$

or by subtracting from the value of 1 the $R^2$ coefficient: 

$$\varphi^2=1-R^2$$

The $\varphi^2$ coefficient of non-determination determines how much of the variance of the dependent variable has not been explained by the regression model.

In our regression model, $\varphi^2=11\%$, which means that 11% of sales variation was not explained by the regression function. We can also say that 11% of sales variation is random factors not explained by the regression model.

The last element of the analysis is to evaluate the quality of the $a_1$ and $a_0$ coefficients. We have determined the regression equation based on the available data, but we don't know the equation for the population. As a result, we may have been a little mistaken in calculating the $a_1$ and $a_0$ parameters. In order to assess the scale of these errors we need to calculate **standard errors of regression coefficient**:

$$S_{a_{1}}=\frac{S_{u}}{\sqrt{\sum\limits_{i=1}^{n}{(x_{i}-\bar{x})^2}}}$$

and 

$$S_{a_{0}}=\sqrt{\frac{S_{u}^{2}\sum\limits_{i=1}^{n}{{x_{i}^2}}}{n\sum\limits_{i=1}^{n}{(x_{i}-\bar{x})^2}}}$$

These errors indicate how much, on average ($+/-$), the regression model parameters deviates from their true values. It is of course desirable that these errors are as small as possible. Therefore, it is assumed that the quantities:

$$V_{a_{1}}=\frac{S_{a_{1}}}{a_{1}}$$

$$V_{a_{0}}=\frac{S_{a_{0}}}{a_{0}}$$

should not exceed 0,5 (50\%) in absolute value. 

This is especially important for the slope $a_1$, while for the intercept $a_0$ this property does not have to be fulfilled.

In our model, the value of the $a_1$ deviates from its real value by $+/-$ 0,21 which is 2% of the value of this parameter. In turn, the value of parameter $a_0$ deviates from its real value by $+/-$ 119,81, which is 11% of this parameter's value.

## Regression in Excel

**First method**

Select the points in the scatter plot and right-click. Select **Add Trendline** and then select Display equation on chart and Display R-squared value on chart.

**Second method**

All the parameters of the regression function can also be determined using the built-in Excel function - LINEST. The syntax is as follows:

- LINEST(known_ys; known_xs; const; stats)

where:

- known_ys - values of dependent variable (y),
- known_xs - values of independent variable (x),
- const - a logical value specifying whether to force the constant to equal 0. Providing 1 estimates model with intercept.
- stats - a logical value specifying whether to return additional regression statistics. If it is equal to 1 

After writing the function and taking into account all arguments, press ENTER - one value should appear. Next, select the area 2 of the column for 5 rows, taking into account the value obtained earlier in the upper left cell. In the next step we go to formula bar and use the keys combination: CTRL+SHIFT+ENTER.

The result is a 2x5 table which contains the following elements:


|                                                         |                                                       |
|---------------------------------------------------------|-------------------------------------------------------|
| slope ($a_{1}$)                       | intercept ($a_{0}$)                                 |
| standard errors of regression coefficient ($S_{a_{1}}$)            | standard errors of regression coefficient ($S_{a_{0}}$)          |
| coefficient of determination ($R^2$)                       | standard error of the regression ($S_{u}$) |
| F statistics ($F$)                                      | degrees of freedom ($n-2$)                         |
| regression sum of squares ($\sum{(\hat{y}-\bar{y})^2}$) | residual sum of squares ($\sum{(y-\hat{y})^2}$)          |


**Third method**

A graphical data analysis environment can also be used to determine the regression parameters. To do this, select the DATA tab and to the right of the DATA ANALYSIS. In the menu, select REGRESSION and click OK. In the input options, select:

- Input Y range - values of dependent variable (y),
- Input X range - values of independent variable (x),
- Labels - if we select also headers.

In output options we can select: output range/new worksheet/new workbook.

As a results we get the following table:

SUMMARY OUTPUT

| Regression Statistics  |         |
|----------------------|---------|
| Multiple R      | $r$     |
| R Square            | $R^2$   |
| Adjusted R Square |         |
| Standard Error     | $S_{u}$ |
| Observations     | $n$     |

ANOVA

|       | df    |    SS       | MS          |F   | Significance F          |
|-----------|-----|-------------|-------------|-------------|-------------|
| Regression  | 1   | $\sum{(\hat{y}-\bar{y})^2}$ |  | $F$ |  |
| Residual | $n-2$ | $\sum{(y-\hat{y})^2}$ |  |             |             |
| Total     | $n-1$ |  |             |             |             |

|        | Coefficients |Standard Error       | t Stat     | P-value   |  
|---------------------|------------------|-------------|--------------|-------------|
| Intercept          | $a_{0}$     | $S_{a_{0}}$ |  |  |
| x variable | $a_{1}$      | $S_{a_{1}}$ |   |  |
