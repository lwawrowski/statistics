---
title: "Descriptive analysis"
author: "≈Åukasz Wawrowski"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(e1071)
knitr::opts_chunk$set(echo = TRUE)
```

# Descriptive statistics

In descriptive analysis of variable we need to calculate few measures:

- measures of average - used to determine that value of the variable described by the distribution around which the remaining values of the variable are concentrated,
- measures of dispersion - for testing the degree of variability of the variable,
- measures of asymmetry - to study asymmetry of distribution,
- measures of concentration - to analyse the degree of concentration of individual units around the average.

There are two types of measures:

- classical measures - calculated based on all observations,
- measures of position - the value of the measure is indicated by the unit.

The aim of the analysis of the structure is to provide a few numbers, which will allow for easy description and comparison of the examined variables.

## Classical measures

The most popular statistic is **arithmetic mean** or **average** definied by formula:

$$\bar{x}=\frac{\sum\limits_{i=1}^{N}{x_{i}}}{N},$$

where: 

- $\bar{x}$ - symbol of arithmetic mean
- $x_{i}$ - values, 
- $N$ - number of observations.

In Excel you can find a function called:

- AVERAGE(values).

The average is a good measure if we want to compare groups. But what if two students have identical average of grades? Does this mean that their grades are also the same? This can happen, but it is quite rare. Below are the grades for two students who have the same average.

```{r s2, echo=FALSE, fig.height=5, fig.width=8, warning=FALSE, message=FALSE}
oceny <- data.frame(id=c(rep("Student 1",7), rep("Student 2",7)),
                    lp=factor(x=rep(c(1:7),2)),
                    oceny=c(c(3,4,5,3.5,2,4,3),c(3.5,4,3,3,3.5,4,3.5)))

ggplot(oceny, aes(x=lp, y=oceny)) + 
  geom_point() +
  geom_hline(yintercept = mean(oceny$oceny), colour = "red") +
  facet_wrap(~ id) +
  scale_y_continuous(breaks = c(2,2.5,3,3.5,4,4.5,5)) +
  xlab("Number of grade") + ylab("Grade") +
  theme_bw()

n <- 7
var1 <- round(((n-1)/n)*as.numeric(var(subset(oceny, id=="Student 1")["oceny"])),2)
var2 <- round(((n-1)/n)*as.numeric(var(subset(oceny, id=="Student 2")["oceny"])),2)
```

What we can see that the grades of student number 2 are closer to the average. The measure of the dispersion of variable is **variance** given by the formula:

$$s^2=\frac{1}{N}\sum\limits_{i=1}^{N}{(x_{i}-\bar{x})^2}$$

where: 

- $s^2$ - symbol of variance, 
- $\bar{x}$ - arithmetic mean, 
- $x_{i}$ - values,
- $N$ - number of observations.

The first student's grades are `r var1` and the second `r var2`. From this measure we are able to conclude that there is a greater variation in the first student's grade. However, we cannot say how much they differ because we are not able to interpret the variance. This is because the result of the variance is given in squared units, which usually makes no sense.

In Excel there are two functions to calculate variance:

- VAR.P (in equation there is $\frac{1}{N}$),
- VAR.S (in equation there is $\frac{1}{N-1}$).

Depending on whether we have population or just a sample, we should use an appropriate formula. During the class we assume that we have the entire population and will use the appropriate functions.

The square root of variance i.e. **standard deviation**, allows to determine the variation numerically. It indicates how much the units differ from the average. In the interpretation of the standard deviation we have to remember about the word _average_ appearing twice. The first concerns the mean used in the formula for variance, and the second defines the arithmetic mean calculated earlier.

```{r s, echo=FALSE, fig.height=5, fig.width=8, warning=FALSE, message=FALSE}
sd1 <- round(sqrt(var1),2)
sd2 <- round(sqrt(var2),2)
```

About the first student we say that his grades differ on average from the mean by `r sd1` grade, while the grades of the second student differ on average by `r sd2` grade.

As previous there are two functions for calcuating standard deviation:

- STDEV.P(values),
- STDEV.S(values).

If the averages are the same, a standard deviation is sufficient to evaluate the differentiation. However, the situation becomes more complicated when there are differences between averages. So how do you compare the variation of variables that have different averages and standard deviations?

An experiment was conducted in which 100 people were measured the length of their arms and legs.

```{r v_s, echo=FALSE, fig.height=5, fig.width=8, warning=FALSE, message=FALSE}
set.seed(100)
n <- 100
rece_nogi <- data.frame(obs=c(rep("Length of arm",100), rep("Length of leg",100)),
                    id=rep(c(1:n),2),
                    dl=c(round(rnorm(n, 74.75, 9.19),2),round(rnorm(n, 102.84, 11.66),2))) 

rn_stat <- rece_nogi %>%
  group_by(obs) %>%
  summarize(sr=mean(dl),
            sd=sqrt(((n-1)/n)*var(dl)),
            sk=skewness(dl),
            ku=kurtosis(dl)+3)

rn <- left_join(rece_nogi, rn_stat)

ggplot(rn, aes(x=id, y=dl)) + 
  geom_point() +
  geom_hline(aes(yintercept = sr), colour = "red") +
  facet_wrap(~ obs) +
  xlab("Number of person") + ylab("Length (in cm)") +
  theme_bw()

```

The average leg length was `r round(rn_stat$sr[1],2)` cm and the standard deviation `r round(rn_stat$sd[1],2)` cm. In turn, the average length of the arm was characterized by the value of `r round(rn_stat$sr[2],2)` cm with a standard deviation of `r round(rn_stat$sd[2],2)` cm. Evaluation of the variation of variables with different averages is possible using **classical coefficient of variation**:

$$CV=\frac{s}{\bar{x}}\cdot 100,$$

where: 

- $s$ - standard deviation, 
- $\bar{x}$ - arithmetic mean.

The coefficient of variation is expressed as a percentage and several contractual thresholds may be adopted:

- 0%-20% - poorly diverse,
- 21%-40% - moderately diverse,
- 41%-60% - strongly diverse,
- over 60% - a very strong diverse.

When calculating the coefficient of variation for leg length, we obtain `r round(rn_stat$sd[1]/rn_stat$sr[1]*100,2)`%, and for arm length `r round(rn_stat$sd[2]/rn_stat$sr[2]*100,2)`%. On this basis, we can conclude that hand length is characterized by greater variation.

The classic coefficient of variation does not have a suitable function programmed in Excel. Instead, you can easily calculate this value.

**The classic asymmetry coefficient** is lso called the third central moment or skewness is expressed by the formula:
$$\alpha_{3}=\frac{\frac{1}{N}\sum\limits_{i=1}^{N}{(x_{i}-\bar{x})^3}}{s^3},$$

where: 

- $\alpha_{3}$ - symbol of asymmetry coefficient,  
- $s$ - standard deviation, 
- $\bar{x}$ - arithmetic mean, 
- $x_{i}$ - values, 
- $N$ - number of observations.

It allows us to determine whether the distribution of a variable is:

- symmetric - the distribution is symmetric, $\alpha_{3}=0$,
- left-skewed - extended left arm of distribution, $\alpha_{3}<0$,
- right-skewed - extended right arm of distribution, $\alpha_{3}>0$.

The skewness of leg length is `r rn_stat$sk[1]`, which means that the distribution of leg length has a slight right-hand asymmetry.

In Excel there are functions called:

- SKEW(values),
- SKEW.P(values).

Focusing around the average defines **the classic concentration coefficent**, also known as the fourth central moment or kurtosis:

$$\alpha_{4}=\frac{\frac{1}{N}\sum\limits_{i=1}^{N}{(x_{i}-\bar{x})^4}}{s^4},$$

where: 

- $\alpha_{4}$ - symbol of concentration coefficent,  
- $s$ - standard deviation, 
- $\bar{x}$ - arithmetic mean, 
- $x_{i}$ - values, 
- $N$ - number of observations.

Allows you to determine whether the distribution of a variable is:

- normal - $\alpha_{4}=3$,
- flat. The values aren't very concentrated around the average, $\alpha_{4}<3$,
- slim. The values are heavily concentrated around the average, $\alpha_{4}>3$.

Some programs instead of kurtosis calculate excess kurtosis:

$Ex=\alpha_{4}-3$

Then we interpret the value of this measure by taking 0 as the reference point.

The curtosis for leg length is `r rn_stat$ku[1]+3`, which means that the distribution of leg length is slim.

In Excel there is a function called:

- KURT(values).

In fact, the result of this function is an excess kurtosis. In the interpretation, therefore, the result is referred to a 0.

## Measures of position

Basic positional measures are not calculated using all observations, as is the case of classical measures, but we look for an observation that indicates the value of the selected positional measure. The most popular positional measure is **median** (quartile 2, middle value, $Q_2$), which determines the value for which 50% of the observations have values lower or equal to the median and 50% equal or greater than the median.

The median is determined by sorting the value of the variable in ascending order and selecting the middle value (if $N$ is odd) or the average of the middle values (if $N$ is even).

The advantage of the median is that it is less sensitive to outlying observations. Consider the case of wages in two companies:

```{r mediana, echo=FALSE, fig.height=5, fig.width=8, warning=FALSE, message=FALSE}

wynagrodzenia <- data.frame(id=c(rep("Company A",14), rep("Company B",14)),
                    lp=factor(x=rep(c(1:14),2)),
                    wyn=c(c(2000,2100,2400,2500,2700,2900,3500,3500,3800,3850,4100,4250,4400,5000),
                          c(2000,2100,2100,2200,2350,2400,2500,2550,2700,2700,4900,4950,5000,5000)))

wyn_stat <- wynagrodzenia %>%
  group_by(id) %>%
  summarise(srednia=round(mean(wyn)),
            mediana=round(median(wyn)),
            min=round(min(wyn)),
            q1=round(quantile(wyn, 0.25)),
            q3=round(quantile(wyn, 0.75)),
            max=round(max(wyn))) %>%
  mutate(q=round((q3-q1)/2),
         vq=round(q/mediana*100),
         aq=round((q1+q3-2*mediana)/(2*q),2))

wyn_stat$id <- factor(wyn_stat$id, levels = rev(unique(wyn_stat$id)), ordered = T)

wynagrodzenia <- left_join(wynagrodzenia, wyn_stat)

ggplot(wynagrodzenia, aes(x=lp, y=wyn)) + 
  geom_point() +
  facet_wrap(~ id) +
  geom_hline(aes(yintercept = srednia), colour = "red") +
  geom_hline(aes(yintercept = mediana), colour = "blue") +
  xlab("Number of employee") + ylab("Salary") +
  theme_bw()

```

In Company A, the salaries of employees are not diverse and there are not too big differences between them. The average salary (red) is `r wyn_stat$mediana[1]` PLN, while the median (blue) corresponds to the salaries of 7 and 8 employees - `r wyn_stat$mediana[1]` PLN. It can be said that both values reflect well the real earnings of employees. In turn, in company B the income inequalities are much greater, it is possible that the salaries of low-level employees and managers were compared. The average of `r wyn_stat$srednia[2]` PLN does not reflect the real earnings of either the first or second group. On the other hand, the median value of `r wyn_stat$mediana[2]` PLN is more resistant to outliers. The median salary in company B means that 50% of the employees receive a salary of `r wyn_stat$mediana[2]` PLN or less, while the other 50% of the employees receive a salary of `r wyn_stat$mediana[2]` PLN or more.

In Excel:

- MEDIAN(values),
- QUARTILE.INC(values, 2).

The median divided our observations into two halves. If we divide the first half into halves again, we get the value of the **first (lower) quartile**, which indicates that 25% of the observations have values lower or equal to the first quartile $Q_{1}$ and 75% equal or higher than this quartile. After dividing the second half of the observations we get the value of the **third (upper) quartile**, which indicates that 75% of the observations have values lower than or equal to the third quartile $Q_{3}$ and 25% equal to or higher than this quartile.

We use a function to determine the quartile values in Excel:

- QUARTILE.INC(values, number of quartile),

where number of quartile:

- 0 - minimum,
- 1 - first quartile,
- 2 - median,
- 3 - third quartile,
- 4 - maximum.

In Company A, the lower quartile of wage was `r wyn_stat$q1[1]` PLN, which means that 25% of employees earn wages equal to or less than `r wyn_stat$q1[1]` PLN, and 75% equal to or more than `r wyn_stat$q1[1]`. Conversely, 75% of employees are paid less than or equal to `r wyn_stat$q3[1]` PLN, and 25% are paid more than or equal to `r wyn_stat$q3[1]` PLN. In Company B, the first quartile equals `r wyn_stat$q1[2]` PLN, and the third `r wyn_stat$q3[2]` PLN.

Quartile values can be shown in the boxplot:

```{r boxplot, echo=FALSE, fig.height=5, fig.width=8, warning=FALSE, message=FALSE}

ggplot(wyn_stat, aes(x = id, ymin = min, lower = q1, middle = mediana, upper = q3, ymax = max)) +
  geom_boxplot(stat = "identity") +
  coord_flip() +
  xlab("") + ylab("Salary") +
  theme_bw()

```

In positional measures, the variation from median measures **quartile deviation**:

$$Q=\frac{(Q_{3}-Q_{1})}{2}$$

where:

- $Q$ - symbol of quartile deviation, 
- $Q_{1}$ - first quartile, 
- $Q_{3}$ - third quartile.

It measures the average deviation of the value of a community feature from the median in 50% of the middle units - between the lower and upper quartile. For example, in company A, the average deviation from the median is `r wyn_stat$q[1]` PLN.

The combination of the quartile deviation and the median allows to calculate the **positional coefficient of variation**:

$$V_{Q}=\frac{Q}{Q_2} \cdot 100$$

where: 

- $V_{Q}$ - symbol of positional coefficient of variation, 
- $Q$ - quartile deviation, 
- $Q_2$ - median.

As in the case of the classic coefficient of variation, we use contractual thresholds. In company A, the positional coefficient of variation was equal to `r wyn_stat$vq[1]`%, which means that salaries in this company were characterised by moderate variation, while in company B it was `r wyn_stat$vq[2]`%, which means a strong variation in salaries.

The last measure based on quartiles is the **positional asymmetry coefficient**, which determines the direction and strength of the asymmetry of units between the first and third quartiles:

$$A_{Q}=\frac{(Q_{1}+Q_{3}-2 \cdot Q_2)}{(2 \cdot Q)}$$

where: 

- $A_{Q}$ - symbol of positional asymmetry coefficient, 
- $Q_{1}$ - first quartile, 
- $Q_{3}$ - third quartile, 
- $Q_{2}$ - median, 
- $Q$ - quartile deviation.

The interpretation of the positional asymmetry coefficent is identical to the classical asymmetry coefficent:

- symmetrical - the median between the values of the lower and upper quartile, $A_{Q}=0$,
- left-skewed - median closer to the upper quartile, $A_{Q}<0$,
- right-skewed - median closer to the lower quartile, $A_{Q}>0$.

This information can also be read from the boxplot, determining the location of the median relative to the other quartiles:

```{r asymetria, echo=FALSE, fig.height=5, fig.width=8, warning=FALSE, message=FALSE}

as <- c("Symmetrical", "Left-skewed", "Right-skewed")
box_sym <- data.frame(x = factor(x = as, levels = rev(as),  ordered = T),
                      min=c(0,0,0),
                      q1=c(15,10,15),
                      q2=c(40,55,30),
                      q3=c(65,65,70),
                      max=c(80,80,80))


ggplot(box_sym, aes(x = x, ymin = min, lower = q1, middle = q2, upper = q3, ymax = max)) +
  geom_boxplot(stat = "identity") +
  coord_flip() +
  xlab("") + ylab("") +
  theme_bw()

```

In company A, the positional asymmetry coefficient was equal to `r wyn_stat$aq[1]`, which informs about left-handed asymmetry, while in company B, there was right-handed asymmetry (`r wyn_stat$aq[2]`).